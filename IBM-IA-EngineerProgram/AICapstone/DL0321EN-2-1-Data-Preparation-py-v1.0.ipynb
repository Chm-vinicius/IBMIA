{"cells":[{"cell_type":"markdown","id":"68b353df-284d-4644-99c5-e9e72a5b1bdd","metadata":{},"source":["<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n","\n","<h1 align=center><font size = 5>Data Preparation</font></h1>\n"]},{"cell_type":"markdown","id":"17b45781-bc60-4628-96f4-4908a5f57b05","metadata":{},"source":["## Objective\n"]},{"cell_type":"markdown","id":"d7a2ee6b-9dbe-4fd2-a45d-44659b65389b","metadata":{},"source":["In this lab, you will learn how to load images and manipulate them for training using Keras ImageDataGenerator.\n"]},{"cell_type":"markdown","id":"8f1c804d-7c7d-437a-8581-a0a341dd479b","metadata":{},"source":["## Table of Contents\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 3>    \n","\n","1. <a href=\"#item22\">Import Libraries and Packages</a> \n","2. <a href=\"#item21\">Download Data</a> \n","3. <a href=\"#item23\">Construct an ImageDataGenerator Instance</a>  \n","4. <a href=\"#item24\">Visualize Batches of Images</a>\n","5. <a href=\"#item25\">Questions</a>    \n","</font>\n","    \n","</div>\n"]},{"cell_type":"markdown","id":"a2308959-403d-48a6-9cab-0cedd37cfa45","metadata":{},"source":["   \n"]},{"cell_type":"markdown","id":"3a888418-b4cc-456d-bbef-07fa9d107c73","metadata":{},"source":["<a id=\"item1\"></a>\n"]},{"cell_type":"markdown","id":"79ee144c-eb65-449c-8cae-73f261ef5ca9","metadata":{},"source":["<a id='item21'></a>\n"]},{"cell_type":"markdown","id":"800ae295-fd58-4a1f-b296-ea132082fd55","metadata":{},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","id":"1acb3501-c22f-46f3-8a81-80d85ccf163a","metadata":{},"source":["Before we proceed, let's import the libraries and packages that we will need to complete the rest of this lab.\n"]},{"cell_type":"code","execution_count":2,"id":"227c04b5-3a15-43a3-9eed-f937c18e57e0","metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import skillsnetwork\n","import keras\n","from keras.preprocessing import image"]},{"cell_type":"markdown","id":"a95ed338-8e4b-4e0e-8b9d-98adec29a94f","metadata":{},"source":["## Download Data\n"]},{"cell_type":"markdown","id":"2f7a808f-17c2-416c-aded-356a74d908c9","metadata":{},"source":["For your convenience, I have placed the data on a server which you can retrieve and unzip easily using the **skillsnetwork.prepare** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed.\n"]},{"cell_type":"code","execution_count":6,"id":"54fecc67-a57d-4ed4-ace2-7c0982750513","metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '\\\\tmp\\\\skills-network--7404449445017894890-concrete_data_week2.zip'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m skillsnetwork\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week2.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m,path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[1;32me:\\pessProjects\\IBMIA\\.venv\\lib\\site-packages\\skillsnetwork\\core.py:249\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(url, path, verbose, overwrite)\u001b[0m\n\u001b[0;32m    246\u001b[0m tmp_download_file \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Download the dataset to tmp_download_file file\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# File will be overwritten if it already exists\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m download(url, tmp_download_file, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Delete extract_dir directory if it already exists\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_jupyterlite():\n","File \u001b[1;32me:\\pessProjects\\IBMIA\\.venv\\lib\\site-packages\\skillsnetwork\\core.py:186\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, path, verbose, chunk_size)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m    185\u001b[0m         path \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Will raise FileNotFoundError if invalid path\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m _get_chunks(url, chunk_size):\n\u001b[0;32m    188\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\tmp\\\\skills-network--7404449445017894890-concrete_data_week2.zip'"]}],"source":["await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week2.zip\",path = \"./\", overwrite=True)"]},{"cell_type":"markdown","id":"ad70e217-6168-4b27-846b-61cbf55a1100","metadata":{},"source":["Now, you should see two folders appear in the left pane: *Positive* and *Negative*. *Negative* is the negative class like we defined it earlier and it represents the concrete images with no cracks. *Positive* on the other hand is the positive class and represents the concrete images with cracks.\n"]},{"cell_type":"markdown","id":"52375065-6589-4400-ac0b-d4bc1015a063","metadata":{},"source":["**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *Negative* and *Positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**.\n"]},{"cell_type":"markdown","id":"ba6f7420-86cd-45fa-854a-c62a5f4322f2","metadata":{},"source":["You can check the content of <code>./concrete_data_week2</code> by running the following:\n"]},{"cell_type":"code","execution_count":7,"id":"2930bd1b-d11f-4dfd-a1eb-1097a3c8881d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["'ls' n�o � reconhecido como um comando interno\n","ou externo, um programa oper�vel ou um arquivo em lotes.\n"]}],"source":["!ls ./concrete"]},{"cell_type":"markdown","id":"08c719b1-9c25-41ae-b15e-bbb1835ec921","metadata":{},"source":["or the following:\n"]},{"cell_type":"code","execution_count":12,"id":"1bfaab24-4829-429f-ae1a-8a09340171cc","metadata":{},"outputs":[{"data":{"text/plain":["['Positive']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir('concrete_data_week2/concrete_data_week2')"]},{"cell_type":"markdown","id":"2d69d897-18d4-4796-908e-ff83dc4ced25","metadata":{},"source":["<a id='item22'></a>\n"]},{"cell_type":"markdown","id":"d946ba63-43e3-4836-982e-54a35efad3d2","metadata":{},"source":[" \n"]},{"cell_type":"markdown","id":"9cc8930d-5a0f-4afc-a1c8-2d816a997aa8","metadata":{},"source":["<a id='item23'></a>\n"]},{"cell_type":"markdown","id":"d151023d-8b8c-4a39-b67b-a6cb76fd1b57","metadata":{},"source":["## Construct an ImageDataGenerator Instance\n"]},{"cell_type":"markdown","id":"7b2ee197-6fba-4d71-b0cb-3cc758bdf54e","metadata":{},"source":["In this section, you will learn how to define a Keras ImageDataGenerator instance and use it to load and manipulate data for building a deep learning model.\n"]},{"cell_type":"markdown","id":"95ffb76a-c18a-4f4d-84bc-c1d0a174cd1b","metadata":{},"source":["Before we proceed, let's define a variable that represents the path to the folder containing our data which is <code>concrete_data_week2</code> in this case.\n"]},{"cell_type":"code","execution_count":13,"id":"d2a3db7b-5227-45af-8809-c47abb8fe065","metadata":{},"outputs":[],"source":["dataset_dir = './concrete_data_week2/concrete_data_week2'"]},{"cell_type":"markdown","id":"52e2b0e7-b152-499c-adcb-afc68a04b8ec","metadata":{},"source":["Keras ImageDataGenerator requires images be arranged in a certain folder hierarchy, where the main directory would contain folders equal to the number of classes in your problem. Since in this case we are trying to build a classifier of two classes, then our main directory, which is <code>concrete_data_week2</code>, should contain two folders, one for each class. This has already been done for you as the negative images are in one folder and the positive images are in another folder.\n"]},{"cell_type":"markdown","id":"49ef1f1d-e437-4fd6-990d-2838cf6cb38b","metadata":{},"source":["Let's go ahead and define an instance of the Keras ImageDataGenerator. \n"]},{"cell_type":"markdown","id":"a20af852-894e-4649-bac2-62c39d1af6cc","metadata":{},"source":["#### Standard ImageDataGenerator\n"]},{"cell_type":"markdown","id":"281057b8-0aba-450b-918e-d3c7cad27c52","metadata":{},"source":["You can define a standard one like this, where you are simply using the ImageDataGenerator to train your model in batches.\n"]},{"cell_type":"markdown","id":"8a52be8a-e4f3-4d7c-a8c6-5ea665c8a98e","metadata":{},"source":["Next, you use the <code>flow_from_directory</code> methods to loop through the images in batches. In this method, you pass the directory where the images reside, the size of each batch, *batch_size*, and since batches are sampled randomly, then you can also specify a random seed, *seed*, if you would like to reproduce the batch sampling. In case you would like to resize your images, then you can using the *target_size* argument to accomplish that.\n"]},{"cell_type":"code","execution_count":15,"id":"891f8612-ea69-42b4-80c7-6c4fd3d50f0e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7235 files belonging to 1 classes.\n"]}],"source":["image_generator = keras.preprocessing.image_dataset_from_directory(\n","    dataset_dir,\n","    batch_size=4,\n","    seed=24\n","    )"]},{"cell_type":"markdown","id":"c843d1e0-518b-464f-b7dd-23b58b398395","metadata":{},"source":["What is great about this method, is it prints a summary of it found in the directory passed. Here, it found 40,000 images in total belonging to 2 classes.\n"]},{"cell_type":"markdown","id":"d68a83ec-dc5b-4456-89a0-a8b53d7713a4","metadata":{},"source":["Now, to access the batches, you use the <code>next</code> method as follows:\n"]},{"cell_type":"code","execution_count":16,"id":"70218eab-0c3b-4860-9e46-2a389c4afcbb","metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'_PrefetchDataset' object has no attribute 'next'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m first_batch \u001b[38;5;241m=\u001b[39m \u001b[43mimage_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m()\n\u001b[0;32m      2\u001b[0m first_batch\n","\u001b[1;31mAttributeError\u001b[0m: '_PrefetchDataset' object has no attribute 'next'"]}],"source":["first_batch = image_generator.\n","first_batch"]},{"cell_type":"markdown","id":"f73dbab9-c6ba-4749-9346-ddd7138e46d8","metadata":{},"source":["As you can see, this returned the images along with their labels. Therefore, the following returns the images only,\n"]},{"cell_type":"code","execution_count":null,"id":"d4cdf262-2b01-47eb-9d8d-20fa5051aa38","metadata":{},"outputs":[],"source":["first_batch_images = image_generator.next()[0]\n","first_batch_images"]},{"cell_type":"markdown","id":"b4dd2050-c90c-4ec4-bb77-e64fdd27d172","metadata":{},"source":["and the following returns the labels only.\n"]},{"cell_type":"code","execution_count":null,"id":"ae21efd5-6e43-409d-8cad-e39851d65677","metadata":{},"outputs":[],"source":["first_batch_labels = image_generator.next()[1]\n","first_batch_labels"]},{"cell_type":"markdown","id":"bb2fd3cf-3c60-4945-9a3b-ecd82a51dd31","metadata":{},"source":["#### Custom ImageDataGenerator\n"]},{"cell_type":"markdown","id":"46eb6c18-667b-4fc7-97bb-d5ed582eb1e8","metadata":{},"source":["You can also specify some transforms, like scaling, rotations, and flips, that you would like applied to the images when you define an ImageDataGenerator object. Say you want to normalize your images, then you can define your ImageDataGenerator instance as follows:\n"]},{"cell_type":"code","execution_count":null,"id":"9f200ff0-fd10-4d8e-833a-7c7df372ed8e","metadata":{},"outputs":[],"source":["# instantiate your image data generator\n","data_generator = ImageDataGenerator(\n","    rescale=1./255\n",")"]},{"cell_type":"markdown","id":"77f7f17a-7e0d-4af3-9edf-01044f383f97","metadata":{},"source":["And then you proceed with defining your *image_generator* using the *flow_from_directory* method, just like before.\n"]},{"cell_type":"code","execution_count":null,"id":"32470637-90de-4f00-a2b9-90bfbbccf12a","metadata":{},"outputs":[],"source":["image_generator = data_generator.flow_from_directory(\n","    dataset_dir,\n","    batch_size=4,\n","    class_mode='categorical',\n","    seed=24\n","    )"]},{"cell_type":"markdown","id":"9c5569fe-a759-4a15-908a-2c18e1239be2","metadata":{},"source":["However, now we explore the first batch using the *next* method, \n"]},{"cell_type":"code","execution_count":null,"id":"5e2972d6-795b-4ccf-ac58-0b814748401f","metadata":{},"outputs":[],"source":["first_batch = image_generator.next()\n","first_batch"]},{"cell_type":"markdown","id":"9de30fec-38dc-4ed2-b48e-acbb7030b79d","metadata":{},"source":["we find that the values are not integer values anymore, but scaled resolution since the original number are divided by 255.\n"]},{"cell_type":"markdown","id":"56c9cf4d-31e6-45df-8c58-2908d0c4ec16","metadata":{},"source":["You can learn more about the Keras ImageDataGeneration class [here](https://keras.io/preprocessing/image/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"]},{"cell_type":"markdown","id":"56a71fed-bd67-4c6b-928e-88cc7b09a937","metadata":{},"source":["<a id='item24'></a>\n"]},{"cell_type":"markdown","id":"da2a0e2c-b392-4d87-93d4-08775eb25289","metadata":{},"source":["## Visualize Batches of Images\n"]},{"cell_type":"markdown","id":"33f3ceee-be31-4f70-960f-c236ee2f51fa","metadata":{},"source":["Let write some code to visualize a batch. We will use subplots in order to make visualizing the images easier.\n"]},{"cell_type":"markdown","id":"4a93334f-6c8c-4d3f-a61e-00a045df7ed2","metadata":{},"source":["Recall that we can access our batch images as follows:\n","\n","<code>first_batch_images = image_generator.next()[0] # first batch</code>\n","\n","<code>second_batch_images = image_generator.next()[0] # second batch</code>\n","\n","and so on.\n"]},{"cell_type":"code","execution_count":null,"id":"3d2e7611-93aa-4981-8376-4d9a7bff2107","metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 10)) # define your figure and axes\n","\n","ind = 0\n","for ax1 in axs:\n","    for ax2 in ax1: \n","        image_data = first_batch_images[ind].astype(np.uint8)\n","        ax2.imshow(image_data)\n","        ind += 1\n","\n","fig.suptitle('First Batch of Concrete Images') \n","plt.show()"]},{"cell_type":"markdown","id":"80e59d5c-cfc8-43f4-bc75-151af7a34a79","metadata":{},"source":["Remember that batches are sampled randomly from the data. In our first batch, we ended up with two negative image and two positive images.\n"]},{"cell_type":"markdown","id":"fabb28f9-b57e-403b-9966-40bf8cc1cb25","metadata":{},"source":["**Important Note**: Because of a bug with the imshow function in Matplotlib, if you are plotting the unscaled RGB images, you have to cast the **image_data** to uint8 before you call the <code>imshow</code> function. So In the code above It looks like this:\n","\n","image_data = first_batch_images[ind].astype(np.uint8)\n"]},{"cell_type":"markdown","id":"3c3fefda-ee24-4f7a-bd7b-66624497b9eb","metadata":{},"source":["<a id='item25'></a>\n"]},{"cell_type":"markdown","id":"1ac5a669-f236-4947-b4ac-534592f9d735","metadata":{},"source":["## Questions\n"]},{"cell_type":"markdown","id":"4ce21775-d6d9-4f53-b745-54d9257c59f5","metadata":{},"source":["### Question: Create a plot to visualize the images in the third batch.\n"]},{"cell_type":"code","execution_count":null,"id":"afd6f9f2-d99e-4879-b03a-5e15d7bc81b9","metadata":{},"outputs":[],"source":["## You can use this cell to type your code to answer the above question\n","\n","\n","\n"]},{"cell_type":"markdown","id":"3ef864e1-12b4-436c-8819-5fc777921c08","metadata":{},"source":["### Question: How many images from each class are in the fourth batch?\n"]},{"cell_type":"code","execution_count":null,"id":"0e482ee9-0893-490c-93f2-72b8e7940459","metadata":{},"outputs":[],"source":["## You can use this cell to type your code to answer the above question\n","\n","\n"]},{"cell_type":"markdown","id":"751ee2ce-b248-4ecb-94dd-062920e9e998","metadata":{},"source":["### Question: Create a plot to visualize the second image in the fifth batch.\n"]},{"cell_type":"code","execution_count":null,"id":"d61d6820-c636-4818-b43d-1843b21a3bd7","metadata":{},"outputs":[],"source":["## You can use this cell to type your code to answer the above question\n","\n","\n"]},{"cell_type":"markdown","id":"ad7fb399-7ab8-4bfa-af99-ee0a7182b850","metadata":{},"source":["### Question: How many images from each class are in the fifth batch?\n"]},{"cell_type":"code","execution_count":null,"id":"25af244d-b5ed-422e-b267-a0fa103f7302","metadata":{},"outputs":[],"source":["## You can use this cell to type your code to answer the above question\n","\n","\n"]},{"cell_type":"markdown","id":"4de00977-4fa9-4394-8b7a-c391da4de059","metadata":{},"source":["   \n"]},{"cell_type":"markdown","id":"90701f41-25cf-4b30-b060-dfc1c89a7b66","metadata":{},"source":["Make sure to answer the above questions as the quiz in this module is heavily based on them.\n"]},{"cell_type":"markdown","id":"f361739c-fc61-4c3d-a49f-34875c73be5f","metadata":{},"source":["  \n"]},{"cell_type":"markdown","id":"76bdcc4f-5d37-45d0-bf51-228de30cb011","metadata":{},"source":["   \n"]},{"cell_type":"markdown","id":"49617b80-61be-4898-bbb9-0659328e26c2","metadata":{},"source":["### Thank you for completing this lab!\n","\n","This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"]},{"cell_type":"markdown","id":"18b7ca60-ce23-48cb-8f24-43de6cfeab17","metadata":{},"source":["This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week2_LAB1).\n"]},{"cell_type":"markdown","id":"ebb86557-4728-42b3-93ae-1b27bf260875","metadata":{},"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n"]},{"cell_type":"markdown","id":"7a55c4df-49ac-492d-bb02-78ebbae73f8b","metadata":{},"source":["<hr>\n","\n","Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":4}
